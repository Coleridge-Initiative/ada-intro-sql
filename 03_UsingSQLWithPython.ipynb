{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SQL and Python Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll go over some code to use an SQL query to bring in a table from a PostgreSQL database into a pandas DataFrame.\n",
    "\n",
    "First, we start as usual by loading the appropriate packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be using the `connect` function in `sqlite3` to connect to the database. Then, we'll use `pandas`, which has some of the SQL reading functionality built into it already, to bring in a table as a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a connection to the database. The code below doesn't actually connect just yet -- we're just creating the engine with which we will connect to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"lodes.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to when we read in CSV files, we're just pointing to a file path containing our database, `lodes.db`. In this case, we have it conveniently located in the same folder as this notebook, so you just need to specify the name of the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to use a different flavor of SQL (such as PostgreSQL), then we would create our connection to the database slightly differently. For example, we could use the `create_engine` function inside the `psycopg2` package to connect to a PostgreSQL database. After you create the connection, though, everything afterwards is the same -- you can use that connection to write SQL code to do all the same things such as bringing in a table as a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading SQL tables using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created our engine to connect to the database, we can use the `read_sql` function in `pandas` to write SQL queries and get tables out as DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM ca_wac_2015\",conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `pd.read_sql()` outputs the table that the SQL query that we wrote as a string would return. In this case, it's simply the `ca_wac_2015` table. Of course, you can include more complicated queries, such as joins, if you'd like.\n",
    "\n",
    "Let's look at the data to make sure we got what we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use more complicated SQL queries as well, bringing in portions of tables. This is more often what you'll want to do, as tables in SQL can get quite large, and you may want data that has already been joined together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT w_geocode, c000, cbsaname, ctyname \n",
    "FROM ca_wac_2015 a\n",
    "JOIN ca_xwalk b\n",
    "ON a.w_geocode = b.tabblk2010;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query,conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are first constructing a query (using the triple quotation marks to create the string over multiple lines for readability), then putting that query into the `read_sql()` function, which outputs the result of the query as a Data Frame. Let's take a quick look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
